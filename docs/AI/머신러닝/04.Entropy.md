# Entropy (엔트로피)

엔트로피는 정보 이론의 핵심 개념으로, 불확실성을 수량화하는 도구이다. 머신러닝에서는 Decision Tree의 최적 분할 기준과 손실 함수로 널리 사용된다.

***

## 1. 정보 엔트로피의 정의

### 1.1 섀넌 엔트로피 (Shannon Entropy)

정보 엔트로피 H는 다음과 같이 정의된다:

$$H(X) = -\sum_{i} p_i \log p_i$$

여기서:
- $p_i$: 각 사건이 일어날 확률
- $-x \log(x)$: 각 사건의 기대 정보 기여도

### 1.2 핵심 함수: $-x \log(x)$

**그래프 특성**:
- $x = 0$ 또는 $x = 1$: 기여도 0 (확실한 경우 정보가 없음)
- $x \approx 0.37$ (1/e 근처): 기여도 최대
- 중간 확률(0.3~0.5): 적당히 불확실할 때 정보량이 최대

**동전 던지기 예시**:

| 상황 | 확률 분포 | 엔트로피 |
|------|-----------|----------|
| 공정한 동전 | p = \{0.5, 0.5\} | H = 1 bit (최대) |
| 편향된 동전 | p = \{0.9, 0.1\} | H ≈ 0.47 bit |
| 양면이 같은 동전 | p = \{1, 0\} | H = 0 bit (확실함) |

***

## 2. 정보량 (Information Content)

### 2.1 정보량의 정의

특정 사건이 일어났다는 소식을 들었을 때 얻는 정보:

$$I(x) = -\log(p)$$

### 2.2 정보량의 직관적 의미

**핵심 원리**: "내 예측이 얼마나 깨졌는가"

- **확률이 낮은 사건 발생** → 놀라움이 큼 → 정보량 많음
- **확률이 높은 사건 발생** → 당연함 → 정보량 적음

**예시: 서울에 눈 3m**

| 상황 | 확률 | 결과를 들었을 때 정보량 |
|------|------|-------------------------|
| "눈 안 왔어" | p ≈ 0.9999 | $-\log(0.9999) \approx 0$ (예상대로) |
| "눈 3m 왔어" | p ≈ 0.0001 | $-\log(0.0001) \approx 13$ (예상 밖) |

### 2.3 정보량의 또 다른 해석

**해석 1: "이 결과를 알려주려면 최소 몇 번의 예/아니오 질문이 필요한가?"**

- 동전 1번 던지기: 가능한 결과 2가지 → 1비트
- 동전 3번 던지기: 가능한 결과 8가지 ($2^3$) → 3비트
- 주사위 던지기: 가능한 결과 6가지 → $\log_2(6) \approx 2.58$ 비트

**해석 2: "이 확률대로 일어날 때 필요한 에너지"**

정보량을 에너지의 관점에서 이해할 수도 있다:
- 정보량 $-\log(p)$ = 확률 p인 사건이 일어날 때 필요한 "에너지"
- 엔트로피 = 각 사건의 정보량 × 확률을 모두 더한 값 = **기대 에너지**
- 즉, $H(X) = \sum p_i \times (-\log p_i)$ = "이 시스템을 운영하는 데 필요한 평균 에너지"

이는 단순한 비유가 아니라 실제로 물리학과 연결된다:
- **란다우어 원리**: 정보 1비트를 지우면 최소 $kT \ln(2)$ 만큼의 열이 발생 (k: 볼츠만 상수, T: 온도)
- 컴퓨터가 뜨거워지는 이유: 계산 = 정보 처리 = 에너지 소비

***

## 3. 왜 로그 함수인가?

### 3.1 섀넌이 정한 정보량의 조건

**조건 1: 확률이 낮으면 정보량이 커야 한다**
- "내일 해가 뜬다" (p ≈ 1) → 당연해서 정보가 없음
- "내일 서울에 눈이 3m 온다" (p ≈ 0) → 일어나면 엄청난 뉴스

**조건 2: 독립 사건의 정보량은 더해져야 한다**
- 동전 두 번 던져서 둘 다 앞면: 확률 $0.5 \times 0.5 = 0.25$
- 정보량: 첫 번째 정보 + 두 번째 정보

### 3.2 로그 함수의 특성

**확률은 곱해지고, 정보량은 더해진다**:

$$-\log(p_1 \times p_2) = -\log(p_1) - \log(p_2)$$

**다른 함수와의 비교**:

| 함수 | 확률 낮으면 커지는가? | 독립사건 더해지는가? |
|------|-----------------------|----------------------|
| $1/x$ | ✓ | ✗ |
| $1/x^2$ | ✓ | ✗ |
| $-\log(x)$ | ✓ | ✓ |

**결론**: 로그만이 두 조건을 모두 만족한다.

### 3.3 왜 더해져야 하는가?

**실용적 이유**:
- 동전 10번 던지기의 정보량 = 1비트 × 10 = 10비트 (직관적)
- 곱셈이면 $2^{10} = 1024$처럼 숫자가 너무 빨리 커짐
- 통신 용량 계산, 데이터 압축 등에서 덧셈이 훨씬 다루기 쉬움

***

## 4. 엔트로피: 평균 정보량

### 4.1 왜 확률과 정보량을 곱하는가?

**$p \times (-\log p)$의 의미**:

- $-\log(p)$: 이 사건이 일어났을 때의 놀라움
- $p$: 그 사건이 실제로 일어날 확률
- **곱**: "이 사건이 평균적으로 나한테 얼마나 놀라움을 줄 것인가"

### 4.2 기대 정보량 계산 예시

**눈 3m 오는 경우**:
- 일어나면 놀라움: $-\log(0.0001) \approx 13$
- 발생 확률: $p = 0.0001$
- 기대 놀라움: $0.0001 \times 13 \approx 0.001$ (작음)

**눈 안 오는 경우**:
- 일어나도 놀라움 없음: $-\log(0.9999) \approx 0.0001$
- 발생 확률: $p = 0.9999$
- 기대 놀라움: $0.9999 \times 0.0001 \approx 0.0001$ (작음)

**동전 앞면 (p=0.5)**:
- 놀라움 중간: $-\log(0.5) = 1$
- 발생 확률: $p = 0.5$
- 기대 놀라움: $0.5 \times 1 = 0.5$ (큼)

### 4.3 엔트로피의 의미

**"이 시스템에서 결과 하나를 알아내려면 평균적으로 질문 몇 번 필요한가?"**

**주사위 예시**:

| 상황 | 확률 분포 | 평균 질문 횟수 |
|------|-----------|----------------|
| 공정한 주사위 | 각 1/6 | 약 2.58번 |
| 조작된 주사위 (1번만 99%) | \{0.99, 0.002, ...\} | 약 1번 (대부분 "1이야?" 한 번에 끝) |

**그래프 $-x \log(x)$가 양 끝에서 0인 이유**:
- **확률 0 근처**: 놀랍긴 한데 안 일어나니까 기대값 낮음
- **확률 1 근처**: 자주 일어나는데 안 놀라우니까 기대값 낮음
- **확률 0.3~0.5**: 적당히 일어나고 적당히 놀라움 → 기대값 최대

***

## 5. 머신러닝 응용: 크로스 엔트로피

### 5.1 크로스 엔트로피 정의

$$H(p, q) = -\sum_{i} p_i \log(q_i)$$

- $p$: 실제 정답 분포
- $q$: 모델의 예측 분포

**의미**: "정답 입장에서 모델의 예측을 믿었을 때 필요한 정보량"

### 5.2 분류 문제 예시

**고양이/개 구분**:
- 정답 (실제): 고양이 → $p = [1, 0]$
- 모델 예측: 고양이 80%, 개 20% → $q = [0.8, 0.2]$
- 크로스 엔트로피: $-1 \times \log(0.8) - 0 \times \log(0.2) \approx 0.32$

**특성**:
- 모델이 정답에 100% 확신 → 크로스 엔트로피 0
- 모델이 틀릴수록 → 크로스 엔트로피 커짐

### 5.3 로지스틱 회귀의 손실 함수

**이진 크로스 엔트로피 (Binary Cross-Entropy)**:

$$L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$$

**정답 $y = 1$ (양성)일 때**:

| 모델 예측 $\hat{y}$ | 손실 |
|---------------------|------|
| 0.99 | $-\log(0.99) \approx 0.01$ |
| 0.8 | $-\log(0.8) \approx 0.22$ |
| 0.5 | $-\log(0.5) \approx 0.69$ |
| 0.1 | $-\log(0.1) \approx 2.3$ |
| 0.01 | $-\log(0.01) \approx 4.6$ |

### 5.4 왜 MSE 대신 크로스 엔트로피를 사용하는가?

**1. 정보 이론적 의미**
- 틀릴수록 페널티가 더 크게 증가
- MSE: $(1 - 0.01)^2 = 0.98$
- 크로스 엔트로피: $-\log(0.01) = 4.6$ (훨씬 큰 페널티)

**2. 수학적 편의성**
- 시그모이드 함수: $\hat{y} = \frac{1}{1 + e^{-z}}$
- 로그를 취하면 지수가 내려와서 미분이 깔끔함
- 경사 하강법 적용이 용이함

**결론**: 크로스 엔트로피는 로지스틱 회귀에 여러 모로 최적의 손실 함수
- 미분이 편하고
- 정보 이론적 의미가 명확하고
- 페널티가 적절함

***

## 6. 핵심 요약

| 개념 | 수식 | 의미 |
|------|------|------|
| 정보량 | $-\log(p)$ | 확률 p인 사건 하나가 주는 정보 |
| 기대 정보 기여도 | $-p \log(p)$ | 정보량 × 발생확률 |
| 엔트로피 | $H(X) = -\sum p_i \log p_i$ | 전체 시스템의 평균 불확실성 |
| 크로스 엔트로피 | $H(p, q) = -\sum p_i \log(q_i)$ | 모델 예측의 손실 |

**엔트로피의 다양한 해석**:
1. 평균적인 놀라움
2. 평균 질문 횟수 (예/아니오)
3. 시스템의 불확실성
4. 데이터 압축의 한계