# Information Gain (정보 이득)

Information Gain은 Decision Tree에서 최적의 분할 기준을 선택하는 핵심 지표이다. 특정 attribute로 데이터를 나누었을 때 불확실성이 얼마나 감소하는지를 측정한다.

***

## 1. Information Gain의 정의

### 1.1 기본 개념

**Information Gain (정보 이득)**:

$$IG = H_{before} - H_{after}$$

- $H_{before}$: 나누기 전 엔트로피
- $H_{after}$: 나눈 후 엔트로피 (가중 평균)
- **의미**: 불확실성이 얼마나 해소되었는가

### 1.2 수식 정의

$$IG = H(parent) - \sum_{i} w_i \cdot H(child_i)$$

여기서:
- $H(parent)$: 부모 노드의 엔트로피
- $w_i$: 각 자식 노드로 간 데이터의 비율
- $H(child_i)$: 각 자식 노드의 엔트로피

***

## 2. 스팸 필터링 예시

### 2.1 초기 상태

**이메일 100개**:
- 스팸: 50개
- 정상: 50개
- 엔트로피: $H = 1$ (반반이니까 불확실)

### 2.2 "무료" 단어로 분류

**분류 후**:

| 그룹 | 스팸 | 정상 | 확률 분포 | 엔트로피 |
|------|------|------|-----------|----------|
| "무료" 있음 (50개) | 45 | 5 | 90% / 10% | 0.47 |
| "무료" 없음 (50개) | 5 | 45 | 10% / 90% | 0.47 |

**가중 평균 엔트로피**:

$$H_{after} = 0.5 \times 0.47 + 0.5 \times 0.47 = 0.47$$

**Information Gain**:

$$IG = 1 - 0.47 = 0.53$$

### 2.3 해석

- 각 그룹이 **거의 확실**해졌다 → 엔트로피가 낮아짐
- 불확실성이 0.53만큼 해소됨
- "무료"는 좋은 분류 기준이다

***

## 3. 엔트로피와 분할

### 3.1 엔트로피는 감소하거나 유지된다

**핵심 원리**: 데이터를 나누면 엔트로피는 **낮아지거나 같음** (절대 증가하지 않음)

**이유**:
- 나누기 전: 전체를 하나로 봄
- 나누기 후: 각 그룹 안에서만 봄
- 어떤 그룹인지 알게 되었으므로 정보가 추가됨
- 최악의 경우: "똑같이 섞여 있음" = 엔트로피 그대로

### 3.2 좋은 분할 vs 나쁜 분할

**좋은 분할 - "무료" 단어**:

| 상태 | 스팸 | 정상 | 엔트로피 |
|------|------|------|----------|
| 나누기 전 | 50 | 50 | 1.0 |
| 무료 있음 | 45 | 5 | 0.47 (확실) |
| 무료 없음 | 5 | 45 | 0.47 (확실) |

→ 엔트로피 낮아짐 ✓

**나쁜 분할 - "이메일 길이가 홀수"**:

| 상태 | 스팸 | 정상 | 엔트로피 |
|------|------|------|----------|
| 나누기 전 | 50 | 50 | 1.0 |
| 홀수 | 25 | 25 | 1.0 (여전히 반반) |
| 짝수 | 25 | 25 | 1.0 (여전히 반반) |

→ 엔트로피 그대로 ✗

**결론**: Information Gain = 0이면 그 기준은 쓸모없다

***

## 4. 확률이 쏠린다는 의미

### 4.1 쏠림의 정의

**"확률이 쏠린다"** = 균등 분포에서 한쪽으로 치우침

**예시**:
- 처음: 50% / 50% (균등)
- "무료" 있는 그룹: 90% / 10% (한쪽으로 쏠림)
- "무료" 없는 그룹: 10% / 90% (반대쪽으로 쏠림)

### 4.2 쏠림과 엔트로피의 관계

**계산 예시**:

| 확률 분포 | 엔트로피 계산 | 값 |
|-----------|---------------|-----|
| 50% / 50% | $-0.5 \log(0.5) - 0.5 \log(0.5)$ | 1.0 |
| 90% / 10% | $-0.9 \log(0.9) - 0.1 \log(0.1)$ | 0.47 |
| 100% / 0% | $-1 \log(1) - 0 \log(0)$ | 0.0 |

**결론**: 쏠릴수록 엔트로피가 낮아진다

### 4.3 왜 쏠리면 엔트로피가 낮아지는가?

**$-x \log(x)$ 그래프의 성질**:

```
값
  |     /‾‾‾\
  |    /     \
  |   /       \
  |__/         \__
  0  0.37  1   x
```

- $x = 0$: 기여도 0
- $x \approx 0.37$ (1/e): 최대
- $x = 1$: 기여도 0
- **가운데가 볼록한 모양**

**이진 분류의 엔트로피 그래프**:

$$H(p) = -p \log(p) - (1-p) \log(1-p)$$

- $p = 0.5$: 엔트로피 최대 (1.0)
- $p = 0$ 또는 $p = 1$: 엔트로피 최소 (0)

**쏠림의 효과**:
- 50% / 50% → 두 항 모두 최댓값 근처
- 90% / 10% → 양쪽 모두 그래프 끝쪽 (값이 작음)
- **0.5에서 멀어질수록** = 쏠릴수록 = 엔트로피 감소

***

## 5. 균등 분포와 최대 엔트로피

### 5.1 엔트로피의 최댓값

**n진 분류에서 엔트로피는 균등 분포일 때 최대**

| 분류 | 균등 분포 | 최대 엔트로피 |
|------|-----------|---------------|
| 2진 | 50% / 50% | $\log(2) = 1$ |
| 3진 | 33% / 33% / 33% | $\log(3) \approx 1.58$ |
| 4진 | 25% / 25% / 25% / 25% | $\log(4) = 2$ |
| n진 | $1/n$ 씩 균등 | $\log(n)$ |

### 5.2 직관적 이해

**균등할 때 가장 불확실하다**:
- 뭐가 나올지 전혀 예측 불가 → 불확실성 최대
- 한쪽으로 쏠리면 → 어느 정도 예측 가능 → 불확실성 감소

### 5.3 수학적 증명: 라그랑주 승수법

**목표**: $H = -\sum_{i} p_i \log(p_i)$를 최대화

**제약 조건**: $\sum_{i} p_i = 1$

**라그랑주 함수**:

$$L = -\sum_{i} p_i \log(p_i) - \lambda \left(\sum_{i} p_i - 1\right)$$

**$p_i$로 미분**:

$$\frac{\partial L}{\partial p_i} = -\log(p_i) - 1 - \lambda = 0$$

$$\log(p_i) = -1 - \lambda$$

**모든 $i$에 대해 동일** → $p_1 = p_2 = \cdots = p_n$

**제약 조건 적용**: $\sum_{i} p_i = 1$이므로 $p_i = \frac{1}{n}$

**결론**: 균등 분포일 때 엔트로피 최대 ✓

***

## 6. 가중 평균과 노드 분할

### 6.1 왜 가중 평균을 사용하는가?

**노드 분할 시**:
- 각 자식 노드의 크기가 다를 수 있음
- 큰 노드의 엔트로피가 전체에 더 큰 영향을 미침
- 따라서 데이터 비율로 **가중 평균**을 계산

### 6.2 가중치 $w_i$의 의미

$$w_i = \frac{\text{자식 노드 i의 데이터 개수}}{\text{부모 노드의 데이터 개수}}$$

**특성**:
- $\sum_{i} w_i = 1$ (가중치 합은 1)
- 노드가 늘어나도 가중치로 인해 엔트로피가 폭발적으로 커지지 않음

### 6.3 예시

**분할 전**: 100개 → $H = 1.0$

**분할 후**:
- 노드 A: 50개 → $w_A = 0.5$, $H_A = 0.47$
- 노드 B: 50개 → $w_B = 0.5$, $H_B = 0.47$

**가중 평균**:

$$H_{after} = 0.5 \times 0.47 + 0.5 \times 0.47 = 0.47$$

**만약 가중치가 없다면**:

$$H_{after} = 0.47 + 0.47 = 0.94$$ (잘못된 계산)

***

## 7. Decision Tree에서의 활용

### 7.1 최적 분할 기준 선택

**알고리즘**:
1. 모든 가능한 attribute에 대해 Information Gain 계산
2. IG가 가장 큰 attribute 선택
3. 해당 attribute로 데이터 분할
4. 각 자식 노드에서 반복

### 7.2 Information Gain 비교

**예시**:

| Attribute | 분할 후 엔트로피 | Information Gain |
|-----------|------------------|------------------|
| "무료" | 0.47 | $1.0 - 0.47 = 0.53$ |
| "안녕" | 0.92 | $1.0 - 0.92 = 0.08$ |
| "이메일 길이" | 1.0 | $1.0 - 1.0 = 0.0$ |

**선택**: "무료" (IG가 가장 큼)

### 7.3 Information Gain의 해석

**IG가 크다** = 불확실성이 많이 해소됨 = 좋은 기준

- **IG = 0**: 쓸모없는 기준 (분할해도 정보 없음)
- **IG가 클수록**: 효율적인 분류 (빠르게 확실해짐)
- **IG 최대화**: 가장 적은 분할로 정확한 분류 달성

***

## 8. 핵심 요약

### 8.1 주요 개념

| 개념 | 의미 |
|------|------|
| Information Gain | 나누기 전 엔트로피 - 나눈 후 엔트로피 |
| 확률이 쏠린다 | 균등 분포 → 한쪽으로 치우침 |
| 가중 평균 | 각 노드의 크기에 비례하여 엔트로피 계산 |
| 최적 분할 | IG가 가장 큰 attribute 선택 |

### 8.2 핵심 원리

1. **엔트로피는 균등할 때 최대**
   - n진 분류: $p_i = 1/n$일 때 $H_{max} = \log(n)$

2. **분할하면 엔트로피는 감소하거나 유지**
   - $H_{after} \leq H_{before}$ (항상)

3. **IG가 크면 좋은 분할**
   - 불확실성을 많이 해소
   - Decision Tree가 빠르게 수렴

4. **가중 평균이 핵심**
   - $\sum w_i = 1$ 보장
   - 노드 크기를 반영한 공정한 계산

### 8.3 Decision Tree 구축 과정

```
1. 현재 노드의 모든 attribute에 대해:
   - 각 attribute로 분할했을 때의 IG 계산

2. IG가 가장 큰 attribute 선택

3. 해당 attribute로 데이터 분할

4. 각 자식 노드가 순수(pure)할 때까지 반복
   - 순수: 모든 데이터가 같은 클래스
   - 또는 IG = 0 (더 이상 나눌 수 없음)
```