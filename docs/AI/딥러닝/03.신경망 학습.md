## 1. 학습 
- 학습 : 데이터에서 학습의 의미는 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻이다.
- 데이터 : 머신러닝에서 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도
- 특징 : 입력 데이터에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기
	- 이미지의 특징은 보통 벡터로 기술, SIFT,SURF,HOG 등의 특징이 있다.
	- 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN등으로 학습할 수 있다.![신경망 학습-20250419130132461.webp](images/%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5-20250419130132461.webp)
	- 훈련 데이터(Training data) / 시험 데이터(Test data) 
		- 이 둘을 나눠야한다. 범용적으로 사용할수 있는 모델이기 때문에. 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로 문제를 올바르게 풀어내는 능력
	- 데이터셋에만 지나치게 최적화된 상태를 과대적합이라고 한다. 
		- 한쪽의 데이터셋이 지나치게 들어가서 편향된 output을 내는 것을 말함. 일종의 편견을 갖게됨.
		- 
***
## 2. 손실 함수
- 손실 함수 : 신경망 상태에서 최적의 매개변수 값(가중치와 편향)을 찾는 것이 중요하다. 주어진 가중치가 편향이 신경망 데이터에서 최적의 매개변수 값인지 판단을 해야한다.(input을 주었을 때 나오는 output이 정답인지 아닌지) 
- 그렇게 하기위해서는 이 매개변수를 정하기위한 **판단근거** 가 있어야 한다. 이 판단 근거, 즉 수치로 표현한 지표를 함수로 나타내고, 이를 손실함수라고 한다. 
- 손실함수는, 주어진 실제값과 매개변수를 통해 계산해서 나온 값을 특수한 수식을 통해 비교하는 함수를 만들고, 매개변수를 어떻게 조정해야 이 계산값이 실제값에 가장 정확하게 나올지를 탐색하기 위한 지표이다.
- 기준이 있어야, 그 데이터의 값의 매개변수를 제대로 정할 수가 있음. 이 기준을 잡는 것이 지표.
- 신경망 학습에서 사용하는 지표를 **손실 함수** 라고 한다. 
- 일반적으로 오차제곱합과 교차 엔트로피 오차를 사용
- 신경망 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄.
#### 오차제곱합
- sum of squares for error(SSE) : 가장 많이 쓰이는 손실 함수
- 예측값과 실제값 사이의 차이를 제곱하여 합산. 
-  수학적 정의 $$ \text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$
	- $n$은 데이터 포인트의 수
	- $y_i$는 $i$번째 실제값 : 신경망의 출력(신경망이 추정한 값)
	- $\hat{y}_i$는 $i$번째 예측값 : 정답 레이블
	- $(y_i - \hat{y}_i)$는 잔차(residual) 또는 오차(error) 
-  벡터 표기법 벡터 $\mathbf{y}$와 $\hat{\mathbf{y}}$에 대해: $$ \text{SSE} = \|\mathbf{y} - \hat{\mathbf{y}}\|^2 = (\mathbf{y} - \hat{\mathbf{y}})^T(\mathbf{y} - \hat{\mathbf{y}}) $$ 평균 제곱 오차 (MSE)와의 관계 평균 제곱 오차(Mean Squared Error, MSE)는 SSE를 데이터 포인트 수로 나눈 것$$ \text{MSE} = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{\text{SSE}}{n} $$ 특징 - 항상 0보다 크거나 같음 (오차가 없을 때 0) - 이상치(outlier)에 민감함 (오차가 제곱되므로) - 미분 가능하므로 경사 하강법을 사용한 최적화에 적합

#### 교차 엔트로피 오차
- cross entropy error(CEE) : 딥러닝에서 분류 문제에 널리 사용되는 손실 함수.
- 모델의 예측 확률 분포와 실제 레이블 분포 사이의 차이를 측정
- 수학적 정의 
	- 단일 데이터 포인트에 대한 교차 엔트로피 오차 $$ E = -\sum_{k} t_k \log(y_k) $$
	- $t_k$는 실제 레이블의 $k$번째 요소 (보통 원-핫 인코딩) 
	- $y_k$는 신경망 출력의 $k$번째 요소 (예측 확률)
	- $k$는 출력 뉴런(또는 클래스)의 인덱스 
- 배치 데이터에 대한 교차 엔트로피 오차 
	- 머신러닝 문제는 훈련 데이터를 사용해 학습 : 훈련 데이터 모두에 대한 손실함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아냄.(모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 함.)
	- $$ E = -\frac{1}{n}\sum_{i=1}^{n}\sum_{k} t_{ik} \log(y_{ik}) $$ 
	- $n$은 배치의 데이터 포인트 수 
	- $t_{ik}$는 $i$번째 데이터의 실제 레이블의 $k$번째 요소 
	- $y_{ik}$는 $i$번째 데이터에 대한 신경망 출력의 $k$번째 요소 
	- 미니배치 : 빅데이터 수준이 되면, 그 수는 수백만에서 수천만도 넘는 거대한 값이 됨. 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 현실적이지 않음. 일부를 추려 전체의 근사치로 이용한다.

- 이진 분류에서의 교차 엔트로피 오차 이진 분류 문제에서는 단순화된 형태로 표현 $$ E = -\frac{1}{n}\sum_{i=1}^{n} [t_i \log(y_i) + (1 - t_i) \log(1 - y_i)] $$
	- $t_i$는 실제 클래스 (0 또는 1) 
	- $y_i$는 양성 클래스에 속할 예측 확률 
- 미분 소프트맥스 함수와 함께 사용될 때의 편미분: $$ \frac{\partial E}{\partial a_j} = y_j - t_j $$ 
	- $a_j$는 소프트맥스 이전의 $j$번째 출력 뉴런 값 
	- $y_j$는 소프트맥스 이후의 $j$번째 확률 값 
	- $t_j$는 $j$번째 클래스의 실제 레이블 값 
- 특징 
- 모델 출력이 실제 레이블에서 멀수록 더 큰 오차 값 제공 
- 소프트맥스 활성화 함수와 함께 사용할 때 계산적으로 효율적인 미분 특성 
- 잘못된 예측에 대해 로그적으로 증가하는 페널티 부여
- 희소 레이블(one-hot 벡터)과 작동하도록 설계됨 
- 최대 가능도 추정(Maximum Likelihood Estimation)과 수학적으로 동등

- 손실함수를 이용한 궁극적인 목적은 정확도를 끌어내는 매개변수 값을 찾는 것. 정확도라는 것은 확률적으로 정해져 있지만, 정답을 향해 반복된 시도를 해야하기 때문에, 손실함수를 이용하는 것이다. 정확히 이 함수의 미분을 통해, 정답을 향해 가까이 가기위해 이러한 함수를 이용하는 것. (정확도의 경우, 확률적으로 정의 되기 때문에, 그 흐름을 알기가 쉽지 않음. 매개변수를 바꾼다고 했을때, 어떻게 바꿔야하는지 알수가 없다. 반면 손실함수의 경우 미분을 통해, 값이 낮아지는 방향을 찾을수가 있어서 흐름을 알 수 있고, 매개변수값을 조정할 수 있다.)

***
## 3. 경사 하강법(gradient decent)
- 경사하강법: 머신러닝 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다. 
- 최적의 매개변수 : 손실 함수가 최솟값이 될 때의 매개변수 값
	- 손실 함수가 최솟값이 라는 뜻은 정답에 가깝게 나오는 최적의 매개변수 값이라는 뜻이다. 
	- 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지 짐작할수 없음.
	- 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 기법
	-  최솟값은 기울기가 0이 되는 값 중 하나이다. 기울기가 낮아지는 방향으로 조금씩 매개변수를 바꾸면, 이 값에 도달할 것 
	- 하지만 기울기가 0이 되는 값이 정말 최솟값인지, 나아갈 방향인지 보장할 수 없음. 그럼에도, 그 방향으로 가야 함수의 값을 줄일 수 있음.
- 손실함수를 만든이유가 최적의 매개변수를 찾기위함이라면, 최적의 매개변수를 찾기위한 방법이 **경사하강법** 이라고 생각하면 된다.
- 경사법을 수식  
	$$ x_0 = x_0-\eta\frac{\partial f}{\partial x_0}$$ $$x_1 = x_1-\eta\frac{\partial f}{\partial x_1}$$
- $\eta$ : 갱신하는 값. **학습률** 이라고 한다. 한 번의 학습으로 기울기 값을 알게 되었고(방향), 최적의 매개변수를 찾기위한 방향으로 얼마만큼 가야하는지를 이 학습률 값이 결정한다. 
- 현재 이 계산은 1회 갱신이고, 이를 여러 단계를 거쳐 반복하면서 서서히 함수의 값을 줄이는 것이다. 
- 학습률이 너무 큰 경우 : 변화값이 너무 커져서, 적절하게 최솟값을 찾을 수 없음. 발산하는 경우도 생겨버린다. 보통 1보다 작은 값으로 설정
- 학습률이 너무 작은 경우 : 변화값이 너무 작아서, 여러번 반복해야함.![신경망 학습-20250426162220403.webp](images/%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5-20250426162220403.webp)
- 학습률 : hyper parameter(초매개변수)라고 함. 가중치와 편향같은 신경망의 매개변수와는 성질이 다른 매개변수이다. 가중치와 편향은 최적의 값을 찾아가는 반면, 이 매개변수는 직접 설정해주어야하는 값이기 때문
- 가중치 매개변수에 대한 손실 함수의 기울기 : 가중치가 $W$, 손실함수가 $L$ 로, 각각의 입력값의 가중치에 대한 손실함수의 기울기를 구해야, 가중치 값을 최적으로 구할 수 있다.
#### SGD(Stochastic Gradient Descent)
- 확률적으로 무작위로 골라낸 데이터를 바탕으로 수행하는 경사 하강법![신경망 학습-20250426164908221.webp](images/%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5-20250426164908221.webp)
- 전체 데이터 셋을 가지고 최솟값의 방향으로 가게 되면, 방향은 올바르지만 데이터양이 너무 많아 매우 느리다.
- 미니배치를 무작위로 선택하여, 각 단계에서의 Gradient Descent를 계산한다.
	- 이렇게 되면 미니배치 값들의 최솟값을 향해 방향(기울기)을 설정하는 것이기 때문에, 이 기울기값이 실제 전체 데이터의 최솟값을 향해 가는 방향(기울기)과 다를 수 있다. 하지만, 한번 받아온 미니배치 데이터셋을 다시 사용하지 않으므로, 결국 반복하다보면 전체 데이터셋의 최솟값을 구하는 방향으로 가게 된다. 즉, 방향은 지그재그로 가게 되지만, 결국 최솟값을 향해 가게 된다.
	- 노이즈가 존재
	- 계산 비용이 전체 데이터셋의 gradient 값을 구하는 것보다 훨씬 적다.
	- 대규모 데이터셋에서 효율적으로 작동
	- 계산 비용이 낮음
	- 지역 최적해에서 빠져나올 가능성이 있다(지역 최적해란,최솟값은 아니지만 기울기가 0이 되는 곳들. )