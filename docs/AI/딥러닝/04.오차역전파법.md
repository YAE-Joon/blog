
## 1.오차역전파

- 오차를 역으로 전파하는 방법(backward propagation of errors)
- 계산 그래프 : 계산과정을 표현한 그래프. Node 와 edge로 표현되며, Node의 방향은 계산 순서대로 진행하게 된다. 이 진행 단계를 순전파(foward prpagation)라고 한다. 이 계산방향을 반대로 가는것을 역전파(backward propgation)이라 한다.
    - 각 노드에서의 계산은 다른 노드와는 영향을 주지 않는 국소적 계산이다. 전체에서 어떻ㄴ 일이 벌어지든 상관업ㅇㅅ이 자신과 관계된 정보만으로 결과를 출력한다.
    - 계산 그래프로 작성시, 중간 계산 결과를 모두 보관할 수 있다.
    - 역전파를 통해 미분을 효율적으로 계산할 수 있다.
        - 전체 결과값에 대해 노드에서의 계산의 변화가 어떤 영향을 끼치고 있는지 알고싶다면, 각 노드에서의 미분값을 구하면 된다는 뜻
        - 역전파를 진행 할때는 반대로 거슬러서 미분하여 값을 계산하므로, 순방향의 입력 신호 값이 필요하다. 곱셈 노드를 구현할 때, 순전파의 입력 신호를 변수에 저장해두어야 한다.

- 코드로 정리
    - 사과와 오렌지를 사는 계산 그래프를 만들것이다. 과일 가격 x 개수 x 세금 으로 계산되고, 각 노드들이 이를 맡는다.
```Python
class MulLayer:

def __init__(self):
self.x = None
self.y = None

def forward(self,x,y):
self.x = x
self.y = y
out = x*y

return out

def backward(self, dout):
dx = dout*self.y
dy = dout*self.x

return dx,dy

class AddLayer:

def __init__(self):
pass

def forward(self,x,y):
out = x+y

return out

def backward(self,dout):
dx = dout *1
dy = dout *1

return dx, dy	
```
- 간단히, input을 xy로 넣고, output을 계산한다. backward로는, input으로 dout을 넣고, dx,dy를 계산하게 된다.( 이는 미분값이다.)
```Python
from layer_naive import MulLayer
from layer_naive import AddLayer

apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax =1.1

  

#계층들
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

  

#순전파
apple_price = mul_apple_layer.forward(apple, apple_num)
orange_price = mul_orange_layer.forward(orange, orange_num)
all_price = add_apple_orange_layer.forward(apple_price,orange_price)
price = mul_tax_layer.forward(all_price,tax)

  

#역전파
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)
dapple_price,dorange_price = add_apple_orange_layer.backward(dall_price)
dapple,dapple_num = mul_apple_layer.backward(dapple_price)
dorange, dorange_num = mul_apple_layer.backward(dorange_price)

  

print(price)
print(dapple_num, dapple, dorange, dorange_num, dtax)
```
- 위의 코드는 실제 값을 넣어 계산하였다. 계산 그래프와 노드를 이용해, 역으로 계산할 수 있게 된다.∙

***
## 2. 활성화 함수 계층 구현
- 실제 Layer는 활성화함수를 이용해서 계층을 표현한다. weight,bios 로 나온 값을 활성화함수로 계산하는 과정을 거치기 때문에, 각각의 구성하는 활성화함수를 클래스 하나로 구현할 수 있다.
#### ReLU 계층
- 활성화 함수로 사용되는 ReLU
    - $$
      y = \begin{cases}
      x & \text( x > 0 )\\
      0 & \text( x \leq 0 )
      \end{cases}
      $$
- 미분하게 되면
    - $$
      \frac{\partial y}{\partial x} = \begin{cases}
      1 & \text( x > 0 )\\
      0 & \text( x \leq 0)
      \end{cases}
      $$
- 코드로 구현하게 되면,
```python
class ReLu:

def __init__(self):
self.mask = None

def forward(self,x):
self.mask = (x<=0)
out = x.copy()
out[self.mask] = 0

return out

def backward(self,dout):
dout[self.mask] = 0
dx = dout

return dx
```

#### Sigmoid 계층
- 활성화 함수로 사용되는 Sigmoid
- $$ y = \frac{1}{1+e^{-x}}$$
- 미분하게 되면,

$$\frac{\partial y}{\partial x} = y(1-y)$$
- 코드로 구현하면,
```python
class Sigmoid:

def __init__(self):
self.out = None

  

def forward(self,x):
out = 1/(1+np.exp(-x))
self.out = out

return out

def backward(self, dout):

dx = dout*(1.0-self.out)*self.out
return dx
```
