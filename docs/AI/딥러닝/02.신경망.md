
- 퍼셉트론으로 컴퓨터가 수행하는 복잡한 함수도 표현할 수 있다.
- 하지만, 가중치(w)를 설정하는 것은 수동으로 함. 이를 해결하기 위해 도입된 개념이다.![신경망-20250330160737983.webp](images/%EC%8B%A0%EA%B2%BD%EB%A7%9D-20250330160737983.webp)
- 이 복잡해 보이는 다층 구조의  퍼셉트론이 신경망의 예시이다.
- 결국, 입력층에서의 값이 함수를 거쳐 변환되어 출력층까지 변환되어 나오게 된다. 
***
## 1. 활성화 함수
- 입력 신호의 총합을 출력 신호로 변환하는 함수를 활성화 함수(activation function)이라고 한다.
- $a = b+w1x1+w2x2$
- $y = h(a)$ 
- 이와 같이, 가중치가 달린 입력 신호와 편향의 총합을 계산한 값을 a라 하고, 이를 h() 함수에 넣어서 y를 출력하게 된다.![[신경망-20250330161936154.webp]]
- 특히, h(a)의 경우 임계값(b)을 경계로 출력이 바뀌는데, 이를 계단 함수(step function)이라고 한다.
- 즉, 퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다고 할 수 있다.  이 계단함수 이외의 함수로 변경을 할 수 있다.
***
## 2. 시그모이드 함수 :
![신경망-20250330162402759.webp](images/%EC%8B%A0%EA%B2%BD%EB%A7%9D-20250330162402759.webp)
![신경망-20250330163908235.webp](images/%EC%8B%A0%EA%B2%BD%EB%A7%9D-20250330163908235.webp)
- 계단함수와 비교하여, 시그모이드 함수는 부드러운 곡선이며, 입력에 따라 출력이 연속적으로 변화한다. 이 매끄러움으로 인해, 퍼셉트론에서는 뉴런사이에 0or1이 흘렀다면, 신경망에서는 연속적인 실수가 흐르게 된다.
- 큰 관점에서 보면 비슷한 모습을 가지고 있다. 입력이 작을 때는 출력이 0에 가깝고, 입력이 커지면 1에 가깝게 되는 구조이다.
- 또한 둘 다 비선형 함수이다. 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. 선형 함수를 이용하면, 신경망의 층을 깊게하는 의미가 없어지기 때문이다. 
	- ex) $h(x)=ax+b$의 경우, 층을 깊게하여 $h(h(h(x)))$ 로 한다고 해도, $a'x+b'$의  구조에서 벗어날 수 없다.
- ReLU(Rectified Linear Unit) 함수 
	- 최근에 주로 이용하는 함수이다. 입력이 0을 넘으면 그 입력을 그대로 출력, 0이하면 0을 출력하는 함수
		- ![신경망-20250330164653140.webp](images/%EC%8B%A0%EA%B2%BD%EB%A7%9D-20250330164653140.webp)
***
## 3. 소프트맥스 함수
- 분류에서 사용하는 함수 
- $$`\text{softmax}(y_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}`$$
- 오버플로우 문제가 존재한다. 지수함수를 사용하는데, 지수 함수의 경우 매우 높은 값을 출력하게 되고, 이런 큰 값끼리 나눗셈을 하게 될 경우 수치가 불안정 해진다.  이를 개선한수식이 있다.
-  소프트맥스 함수 개선 과정

	 1. 기본 소프트맥스 함수
	
		- 원래 소프트맥스 함수
	$$`\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}`$$
	
	2. 분모와 분자에 상수 $C$ 곱하기
	$$`\text{softmax}(z_i) = \frac{C \cdot e^{z_i}}{C \cdot \sum_{j=1}^{n} e^{z_j}} = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}`$$
	
	 3. $`C = e^{\log C}`$ 치환
	$$`\text{softmax}(z_i) = \frac{e^{\log C} \cdot e^{z_i}}{e^{\log C} \cdot \sum_{j=1}^{n} e^{z_j}}`$$
	
	 4. 지수 법칙 적용
	
		- 지수 법칙 $`e^a \cdot e^b = e^{a+b}`$를 적용
	
	$$`\text{softmax}(z_i) = \frac{e^{z_i + \log C}}{\sum_{j=1}^{n} e^{z_j + \log C}}`$$
	 5. $`\log C = C_1`$ 치환
	
		- $\log C$를 새로운 상수 $C_1$로 치환
	
	$$`\text{softmax}(z_i) = \frac{e^{z_i + C_1}}{\sum_{j=1}^{n} e^{z_j + C_1}}`$$

- C1 에 e^z 값중 가장 큰 값을 빼주게 되면, 최대값이 0이 되고, 여기를 기준으로 값들이 평준화 된다.무한대로 치솟는 값을 억제할 수 있다.
- 소프트맥스 함수의 특징
	- 0~1 사이의 값이다. 출력값의 총합이 1 이기 때문
	- 출력을 확률로 해석할 수 있다. 
	- 원래 함수와 비교해서 대소가 바뀌지 않는다(단조 증가 함수)
	- 분류 에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식
	- 소프트맥스 함수를 적용해도, 출력이 가장 큰 뉴런의 위치는 달라지지 않음.
	- 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 됨. 현업에서도 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수는 생략함. 
 - `argmax()` : 축을 기준으로 최댓값의 index를 가져오는 함수로, 소프트맥스 함수와 함께 자주 쓰인다.
	 - `axis` : 축을 어떻게 할지 결정함. 0인경우 0차원, 1의 경우 1차원을 축으로 기준으로 잡는다.

 