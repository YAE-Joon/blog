# Decision Tree 구현


### 1 데이터 로드

```python
# Iris 데이터셋 컬럼명
col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

# CSV 파일 읽기
data = pd.read_csv('iris.csv', skiprows=1, names=col_names)
data.head(10)
```

**Iris 데이터셋**:
- 꽃받침 길이 (sepal_length)
- 꽃받침 너비 (sepal_width)
- 꽃잎 길이 (petal_length)
- 꽃잎 너비 (petal_width)
- 타겟: 꽃의 종류 (3가지)

***

## 2. Node 클래스 구현

### 2.1 Node 클래스 정의

```python
class Node():
    def __init__(self, feature_index=None, threshold=None, left=None, right=None,
                 info_gain=None, value=None):
        """
        Decision Tree의 노드를 표현하는 클래스

        Decision Node (내부 노드)용 속성:
        - feature_index: 분할에 사용할 feature의 인덱스
        - threshold: 분할 기준값
        - left: 왼쪽 자식 노드
        - right: 오른쪽 자식 노드
        - info_gain: 이 분할의 Information Gain

        Leaf Node (말단 노드)용 속성:
        - value: 예측값 (클래스)
        """
        self.feature_index = feature_index
        self.threshold = threshold
        self.left = left
        self.right = right
        self.info_gain = info_gain
        self.value = value
```

### 2.2 노드의 두 가지 유형

**1. Decision Node (내부 노드)**:
- 데이터를 분할하는 노드
- `feature_index`, `threshold`, `left`, `right`, `info_gain` 사용
- 예: `"petal_length <= 2.5인가?"`

**2. Leaf Node (말단 노드)**:
- 최종 예측을 하는 노드
- `value` 사용 (클래스 레이블)
- 예: "Setosa"

***

## 3. DecisionTreeClassifier 클래스

### 3.1 초기화

```python
class DecisionTreeClassifier():
    def __init__(self, min_samples_split=2, max_depth=2):
        """
        Decision Tree 분류기

        Parameters:
        - min_samples_split: 노드를 분할하기 위한 최소 샘플 수
        - max_depth: 트리의 최대 깊이 (overfitting 방지)
        """
        # 루트 노드
        self.root = None

        # 중지 조건 (stopping conditions)
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
```

**중지 조건의 역할**:
- `min_samples_split`: 샘플이 너무 적으면 분할 중지 (과적합 방지)
- `max_depth`: 트리가 너무 깊어지는 것을 방지 (과적합 방지)

***

## 4. 트리 구축 (build_tree)

### 4.1 재귀적 트리 구축

```python
def build_tree(self, dataset, curr_depth=0):
    """
    재귀적으로 Decision Tree를 구축

    Parameters:
    - dataset: 현재 노드의 데이터 (X와 Y가 합쳐진 형태)
    - curr_depth: 현재 깊이

    Returns:
    - Node 객체 (Decision Node 또는 Leaf Node)
    """
    # X는 features, Y는 target
    X, Y = dataset[:, :-1], dataset[:, -1]
    num_samples, num_features = X.shape

    # 중지 조건 확인
    if num_samples >= self.min_samples_split and curr_depth < self.max_depth:
        # 최적의 분할 방법 찾기
        best_split = self.get_best_split(dataset, num_samples, num_features)

        # Information Gain이 0보다 크면 (아직 순수 노드가 아니면)
        if best_split['info_gain'] > 0:
            # 왼쪽 서브트리 재귀 생성
            left_subtree = self.build_tree(best_split['dataset_left'], curr_depth + 1)
            # 오른쪽 서브트리 재귀 생성
            right_subtree = self.build_tree(best_split['dataset_right'], curr_depth + 1)

            # Decision Node 반환
            return Node(best_split["feature_index"], best_split["threshold"],
                       left_subtree, right_subtree, best_split["info_gain"])

    # Leaf Node 생성 (중지 조건 만족 또는 순수 노드)
    leaf_value = self.calculate_leaf_value(Y)
    return Node(value=leaf_value)
```

### 4.2 트리 구축 과정

```
1. 현재 노드의 데이터 확인
   ↓
2. 중지 조건 확인
   - 샘플 수가 min_samples_split 미만?
   - 깊이가 max_depth 이상?
   ↓ (중지 조건 만족 시 Leaf Node 생성)

3. 최적 분할 찾기 (get_best_split)
   ↓
4. Information Gain > 0?
   ↓ (yes)
5. 왼쪽/오른쪽 자식 노드 재귀 생성
   ↓
6. Decision Node 반환
```

***

## 5. 최적 분할 찾기 (get_best_split)

### 5.1 코드 구현

```python
def get_best_split(self, dataset, num_samples, num_features):
    """
    Information Gain이 최대인 분할 방법 찾기

    Returns:
    - best_split: 최적 분할 정보를 담은 딕셔너리
    """
    best_split = {}
    max_info_gain = -float('inf')  # 음의 무한대로 초기화

    # 모든 feature에 대해 반복
    for feature_index in range(num_features):
        # 해당 feature의 모든 값
        feature_values = dataset[:, feature_index]
        # 중복 제거한 고유값들 (가능한 threshold들)
        possible_thresholds = np.unique(feature_values)

        # 각 threshold에 대해 반복
        for threshold in possible_thresholds:
            # 데이터 분할
            dataset_left, dataset_right = self.split(dataset, feature_index, threshold)

            # 양쪽 모두 데이터가 있어야 함
            if len(dataset_left) > 0 and len(dataset_right) > 0:
                # 부모, 왼쪽 자식, 오른쪽 자식의 Y값
                y = dataset[:, -1]
                left_y = dataset_left[:, -1]
                right_y = dataset_right[:, -1]

                # Information Gain 계산
                curr_info_gain = self.information_gain(y, left_y, right_y)

                # 최댓값 갱신
                if curr_info_gain > max_info_gain:
                    best_split["feature_index"] = feature_index
                    best_split["threshold"] = threshold
                    best_split["info_gain"] = curr_info_gain
                    best_split["dataset_left"] = dataset_left
                    best_split["dataset_right"] = dataset_right
                    max_info_gain = curr_info_gain

    return best_split
```

### 5.2 알고리즘 단계

```
1. 각 feature에 대해:
   ├─ 2. 해당 feature의 고유값들을 threshold 후보로 사용
   │   ├─ 3. 각 threshold로 데이터 분할
   │   ├─ 4. Information Gain 계산
   │   └─ 5. 최댓값 갱신
   └─ 반복

6. IG가 가장 큰 (feature, threshold) 반환
```

***

## 6. 데이터 분할 (split)

### 6.1 코드 구현

```python
def split(self, dataset, feature_index, threshold):
    """
    threshold를 기준으로 데이터를 왼쪽/오른쪽으로 분할

    Parameters:
    - dataset: 분할할 데이터
    - feature_index: 분할 기준 feature의 인덱스
    - threshold: 분할 기준값

    Returns:
    - dataset_left: threshold 이하인 데이터
    - dataset_right: threshold 초과인 데이터
    """
    dataset_left = np.array([row for row in dataset
                            if row[feature_index] <= threshold])
    dataset_right = np.array([row for row in dataset
                             if row[feature_index] > threshold])
    return dataset_left, dataset_right
```

### 6.2 분할 예시

```
Original dataset (petal_length 기준, threshold=2.5):
[1.4, 0.2, ...] → Left  (1.4 <= 2.5)
[4.7, 1.4, ...] → Right (4.7 > 2.5)
[1.3, 0.2, ...] → Left  (1.3 <= 2.5)
[5.1, 2.4, ...] → Right (5.1 > 2.5)
```

***

## 7. Information Gain 계산

### 7.1 코드 구현

```python
def information_gain(self, parent, l_child, r_child, mode="entropy"):
    """
    Information Gain 계산

    IG = H(parent) - [w_L * H(left) + w_R * H(right)]

    Parameters:
    - parent: 부모 노드의 Y값
    - l_child: 왼쪽 자식 노드의 Y값
    - r_child: 오른쪽 자식 노드의 Y값
    - mode: "entropy" 또는 "gini"
    """
    # 가중치 계산
    weight_l = len(l_child) / len(parent)
    weight_r = len(r_child) / len(parent)

    if mode == "gini":
        # Gini Index 사용
        gain = self.gini_index(parent) - (
            weight_l * self.gini_index(l_child) +
            weight_r * self.gini_index(r_child)
        )
    else:
        # Entropy 사용 (기본값)
        gain = self.entropy(parent) - (
            weight_l * self.entropy(l_child) +
            weight_r * self.entropy(r_child)
        )
    return gain
```

### 7.2 수식

$$IG = H(parent) - \left[w_L \cdot H(left) + w_R \cdot H(right)\right]$$

여기서:
- $w_L = \frac{\text{left 샘플 수}}{\text{parent 샘플 수}}$
- $w_R = \frac{\text{right 샘플 수}}{\text{parent 샘플 수}}$

***

## 8. Entropy 계산

### 8.1 코드 구현

```python
def entropy(self, y):
    """
    엔트로피 계산

    H(Y) = -Σ p(c) * log2(p(c))

    Parameters:
    - y: 클래스 레이블 배열

    Returns:
    - entropy: 엔트로피 값
    """
    class_labels = np.unique(y)  # 고유한 클래스들
    entropy = 0

    for cls in class_labels:
        # 클래스 c의 비율
        p_cls = len(y[y == cls]) / len(y)
        # -p * log2(p) 누적
        entropy += -p_cls * np.log2(p_cls)

    return entropy
```

### 8.2 계산 예시

```python
# y = [0, 0, 1, 1, 1]
# 클래스 0: 2개 (40%)
# 클래스 1: 3개 (60%)

H(Y) = -(0.4 * log2(0.4) + 0.6 * log2(0.6))
     = -(0.4 * -1.32 + 0.6 * -0.74)
     = -(-0.528 - 0.444)
     = 0.972
```

***

## 9. Gini Index 계산

### 9.1 코드 구현

```python
def gini_index(self, y):
    """
    지니 계수 계산

    Gini = 1 - Σ p(c)²

    Parameters:
    - y: 클래스 레이블 배열

    Returns:
    - gini: 지니 계수 값
    """
    class_labels = np.unique(y)
    gini = 0

    for cls in class_labels:
        # 클래스 c의 비율
        p_cls = len(y[y == cls]) / len(y)
        # p² 누적
        gini += p_cls ** 2

    return 1 - gini
```

### 9.2 Entropy vs Gini Index

| 지표 | 수식 | 특징 |
|------|------|------|
| Entropy | $H = -\sum p_i \log_2(p_i)$ | 정보 이론 기반, 로그 연산 |
| Gini Index | $G = 1 - \sum p_i^2$ | 제곱 연산, 계산 빠름 |

**공통점**: 둘 다 불순도(impurity)를 측정, 낮을수록 순수

***

## 10. Leaf Node 값 계산

### 10.1 코드 구현

```python
def calculate_leaf_value(self, y):
    """
    Leaf Node의 예측값 결정 (다수결)

    Parameters:
    - y: 해당 노드의 클래스 레이블들

    Returns:
    - 가장 많이 등장한 클래스
    """
    Y = list(y)
    return max(Y, key=Y.count)  # 최빈값 반환
```

### 10.2 예시

```python
# y = [0, 0, 0, 1, 1]
# 클래스 0: 3개
# 클래스 1: 2개
# → Leaf value = 0 (다수결)
```

***

## 11. 트리 출력 (print_tree)

### 11.1 코드 구현

```python
def print_tree(self, tree=None, indent=" "):
    """
    트리 구조를 재귀적으로 출력

    Parameters:
    - tree: 출력할 노드 (기본값: root)
    - indent: 들여쓰기
    """
    if not tree:
        tree = self.root

    # Leaf Node인 경우
    if tree.value is not None:
        print(tree.value)
    # Decision Node인 경우
    else:
        print("X_" + str(tree.feature_index), "<=", tree.threshold,
              "?", tree.info_gain)
        print("%sleft:" % indent, end="")
        self.print_tree(tree.left, indent + indent)
        print("%sright:" % indent, end="")
        self.print_tree(tree.right, indent + indent)
```

### 11.2 출력 예시

```
X_2 <= 2.5 ? 0.95
 left: 0
 right: X_3 <= 1.75 ? 0.42
   left: 1
   right: 2
```

**해석**:
- Root: petal_length(X_2) <= 2.5로 분할 (IG=0.95)
- Left: 클래스 0 (Setosa)
- Right: petal_width(X_3) <= 1.75로 분할 (IG=0.42)
  - Left: 클래스 1 (Versicolor)
  - Right: 클래스 2 (Virginica)

***

## 12. 학습 및 예측

### 12.1 학습 (fit)

```python
def fit(self, X, Y):
    """
    Decision Tree 학습

    Parameters:
    - X: features (n_samples, n_features)
    - Y: targets (n_samples, 1)
    """
    # X와 Y를 합쳐서 dataset 생성
    dataset = np.concatenate((X, Y), axis=1)
    # 트리 구축
    self.root = self.build_tree(dataset)
```

### 12.2 예측 (predict)

```python
def predict(self, X):
    """
    예측 수행

    Parameters:
    - X: 예측할 데이터

    Returns:
    - predictions: 예측된 클래스 레이블들
    """
    predictions = [self.make_prediction(x, self.root) for x in X]
    return predictions

def make_prediction(self, x, tree):
    """
    단일 샘플에 대한 예측 (재귀)

    Parameters:
    - x: 하나의 샘플
    - tree: 현재 노드

    Returns:
    - 예측 클래스
    """
    # Leaf Node에 도달
    if tree.value != None:
        return tree.value

    # Decision Node: 조건 확인
    feature_val = x[tree.feature_index]
    if feature_val <= tree.threshold:
        return self.make_prediction(x, tree.left)
    else:
        return self.make_prediction(x, tree.right)
```

### 12.3 예측 과정

```
1. Root 노드에서 시작
   ↓
2. Decision Node인가?
   ↓ (yes)
3. feature_value <= threshold?
   ├─ yes → 왼쪽 자식으로 이동
   └─ no → 오른쪽 자식으로 이동
   ↓
4. 2번으로 돌아가 반복
   ↓
5. Leaf Node 도달 → value 반환
```

***

## 13. 전체 실행 코드

### 13.1 데이터 준비

```python
# 데이터 로드
X = data.iloc[:, :-1].values  # Features
Y = data.iloc[:, -1].values.reshape(-1, 1)  # Target

# Train/Test 분할
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42
)
```

### 13.2 모델 학습 및 출력

```python
# Decision Tree 생성
classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)

# 학습
classifier.fit(X_train, Y_train)

# 트리 구조 출력
classifier.print_tree()
```

### 13.3 예측

```python
# 예측 수행
predictions = classifier.predict(X_test)

# 정확도 계산
accuracy = np.mean(predictions == Y_test.flatten())
print(f"Accuracy: {accuracy * 100:.2f}%")
```

***

## 14. 핵심 요약

### 14.1 주요 구성 요소

| 구성 요소 | 역할 |
|-----------|------|
| Node | Decision Node / Leaf Node 표현 |
| build_tree | 재귀적 트리 구축 |
| get_best_split | IG 최대인 분할 찾기 |
| split | 데이터 분할 |
| information_gain | IG 계산 |
| entropy | 엔트로피 계산 |
| gini_index | 지니 계수 계산 |
| calculate_leaf_value | 다수결로 예측값 결정 |
| fit | 학습 |
| predict | 예측 |

### 14.2 알고리즘 흐름

```
1. fit() 호출
   ↓
2. build_tree() 재귀 시작
   ↓
3. get_best_split()으로 최적 분할 찾기
   ├─ 모든 (feature, threshold) 조합 시도
   ├─ information_gain() 계산
   └─ 최댓값 선택
   ↓
4. split()으로 데이터 분할
   ↓
5. 왼쪽/오른쪽 자식에 대해 build_tree() 재귀
   ↓
6. 중지 조건 만족 시 Leaf Node 생성
   ↓
7. 트리 완성
```

### 14.3 중지 조건

- `num_samples < min_samples_split`: 샘플 수 부족
- `curr_depth >= max_depth`: 최대 깊이 도달
- `info_gain == 0`: 순수 노드 (더 이상 나눌 필요 없음)

### 14.4 Overfitting 방지

- `min_samples_split`: 최소 샘플 수 제한
- `max_depth`: 최대 깊이 제한
- 두 파라미터를 조절하여 모델 복잡도 제어